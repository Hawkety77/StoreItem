clear()
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
library(patchwork)
bodyfat <- vroom('BodyFat.txt') %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19, lower.panel = NULL)
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
library(patchwork)
bodyfat <- vroom('BodyFat.txt') %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19, lower.panel = NULL)
corrplot(cor(bodyfat), type = "upper")
bodyfat.lm <- lm(brozek ~ ., data = bodyfat)
summary(bodyfat.lm)
bodyfat$residuals <- bodyfat.lm$residuals
# residual vs. predictor plots
resid_vs_age <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = age, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_weight <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = weight, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_height<- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = height, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_neck <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = neck, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_chest <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = chest, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_abdom <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = abdom, y = residuals)) +
theme(aspect.ratio = 1)
# put plots in 2 rows & 3 columns using the patchwork package
(resid_vs_age | resid_vs_weight | resid_vs_height) /
(resid_vs_neck | resid_vs_chest | resid_vs_abdom)
# partial regression plots
avPlots(bodyfat.lm)
# residual vs fitted values
autoplot(bodyfat.lm, which = 1)
# Diagnostic 1
hist(bodyfat$brozek)
# Diagnostic 2
autoplot(bodyfat.lm, which = 2)
# Diagnostic 3
shapiro.test(bodyfat$residuals)
autoplot(bodyfat.lm, which = 3)
autoplot(bodyfat.lm, which = 1)
# Cook's Distance
plot(bodyfat.lm, which = 5, cook.levels = c(4/249))
vif(bodyfat.lm)
# load packages here
library(multcomp)
library(tidyverse)
confint(life.lm)
# load packages here
library(multcomp)
library(tidyverse)
life <- read.table('LifeExpectancy.txt', header = TRUE) %>%
dplyr::select(-Row) %>%
dplyr::mutate(Group = as.factor(Group))
summary(life)
ggplot(data = life) +
geom_point(aes(y = LifeExp, x = PPGDP))
ggplot(data = life) +
geom_boxplot(aes(y = LifeExp, x = Group))
ggplot(data = life) +
geom_point(aes(x = PPGDP, y = LifeExp, color = Group))
life$africa <- ifelse(life$Group == 'africa', 1, 0)
life$oecd <- ifelse(life$Group == 'oecd', 1, 0)
colnames(life)
life.lm <- lm(LifeExp ~ ., data = dplyr::select(life, -Country, -Group))
summary(life.lm)
life <- life %>%
mutate(Group = fct_relevel(Group, "other"))
life.lm <- lm(LifeExp ~ ., data = dplyr::select(life, -africa, -oecd, -Country))
summary(life.lm)
confint(life.lm)
# your code here
# your code here
# your code here
# your code here
# your code here
# your code here
# your code here
# your code here
# your code here
full_model <- life.lm
reduced_model <- lm(LifeExp ~ Group, data = life)
anova(full_model, reduced_model)
full_model <- life.lm
reduced_model <- lm(LifeExp ~ PPGDP, data = life)
anova(full_model, reduced_model)
install.packages(tableone)
install.packages('tableone')
library(tidyverse)
library(vroom)
df <- vroom('covid_trust_data.csv')
# questions <- df[1, ]
# questions <- t(questions)
# colnames(questions) <- 'questions'
# Cleaning Covid Data
df_clean <- df %>%
slice(-2, -1) %>%
filter(Finished != 0) %>%
select(-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13,
-16, -17,
-Q2_4_TEXT, -Q3_7_TEXT,
-opp, -qpmid, -RISN, -rid, -V, -cintid, -ProjectToken, -viga, -gc, -term, -Q_TotalDuration, -race, -LS) %>%
rename(age = Q1,
gender = Q2,
race = Q3,
income = Q4,
house_size = Q5,
degree = Q9,
marital_status = Q10,
children = Q11,
political_view = Q12,
living_area = Q14
)
write_csv(df_clean, 'covid_clean.csv')
CreateTableOne(data = select(df_clean, Q31, income, degree))
library(tidyverse)
library(vroom)
library(tableone)
df <- vroom('covid_trust_data.csv')
# questions <- df[1, ]
# questions <- t(questions)
# colnames(questions) <- 'questions'
# Cleaning Covid Data
df_clean <- df %>%
slice(-2, -1) %>%
filter(Finished != 0) %>%
select(-1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13,
-16, -17,
-Q2_4_TEXT, -Q3_7_TEXT,
-opp, -qpmid, -RISN, -rid, -V, -cintid, -ProjectToken, -viga, -gc, -term, -Q_TotalDuration, -race, -LS) %>%
rename(age = Q1,
gender = Q2,
race = Q3,
income = Q4,
house_size = Q5,
degree = Q9,
marital_status = Q10,
children = Q11,
political_view = Q12,
living_area = Q14
)
write_csv(df_clean, 'covid_clean.csv')
CreateTableOne(data = select(df_clean, Q31, income, degree))
df = select(df_clean, Q31, income, degree)
CreateTableOne(vars = 'income', strata = 'Q31', data = df)
df = select(df_clean, Q31, income, degree)
CreateTableOne(vars = 'Q31', strata = 'income', data = df)
df = select(df_clean, Q31, income, degree)
CreateTableOne(vars = c('income', 'degree'), strata = 'Q31', data = df)
df = select(df_clean, Q31, income, degree) %>%
mutate(Q31 = ifelse(Q31 == 1, 'True', 'False')) %>%
rename(`Q31: Most people can be trusted` = Q31)
CreateTableOne(vars = c('income', 'degree'), strata = 'Q31', data = df)
df = select(df_clean, Q31, income, degree) %>%
mutate(Q31 = ifelse(Q31 == 1, 'True', 'False')) %>%
rename(`Q31: Most people can be trusted` = Q31)
CreateTableOne(vars = c('income', 'degree'), strata = 'Q31: Most people can be trusted', data = df)
# load packages here
library(tidyverse)
library(corrplot)  # for the correlation matrix
library(bestglm)  # for variable selection
library(car)  # for the VIFs
library(pROC)  # for the ROC curve
library(ROCR)  # for the color-coded ROC curve
library(vroom)
library(glmnet)  # for ridge, lasso, and elastic net
heart <- vroom('CHD.csv') %>%
mutate(chd = as.factor(chd))
# Jittered Scatterplot for age
ggplot(data = heart) +
geom_point(mapping = aes(y = chd, x = age)) +
geom_jitter(mapping = aes(y = chd, x = age),
height = 0.1) +
theme(aspect.ratio = 1)
# Boxplot for age
ggplot(data = heart) +
geom_boxplot(mapping = aes(y = age, x = chd)) +
theme(aspect.ratio = 1) +
coord_flip()
# Cross-Tabulation (Contingency Table) for age
table(heart$age, heart$chd)
table(heart$age)
corrplot(cor(heart[, -8]))
# For illustration, we will use best subsets (exhaustive) with the BIC metric
heart_best_subsets_bic <- bestglm(as.data.frame(heart),
IC = "BIC",
method = "exhaustive",
TopModels = 1,
family = binomial)
summary(heart_best_subsets_bic$BestModel)
set.seed(12345)  # make sure to set your seed when doing cross validation!
heart$chd_num <- ifelse(heart$chd == "yes", 1, 0)
env_x <- as.matrix(heart[, 1:7])
env_y <- heart$chd_num
# use cross validation to pick the "best" (based on MSE) lambda
env_lasso_cv <- cv.glmnet(x = env_x,
y = env_y,
type.measure = "mse",
alpha = 1)  # 1 is code for "LASSO"
coef(env_lasso_cv, s = "lambda.min")
heart_logistic <- glm(chd ~ age + weight + sbp + chol + cigs, data = heart, family = binomial(link = 'logit'))
summary(heart_logistic)
# here is the plot for age - repeat for the other predictors
scatter.smooth(x = heart$age, y = as.numeric(heart$chd) - 1)
scatter.smooth(x = heart$weight, y = as.numeric(heart$chd) - 1)
scatter.smooth(x = heart$sbp, y = as.numeric(heart$chd) - 1)
scatter.smooth(x = heart$chol, y = as.numeric(heart$chd) - 1)
scatter.smooth(x = heart$cigs, y = as.numeric(heart$chd) - 1)
# this code uses the pseudo R-Squared
vif(heart_logistic)
# this code uses the real R-Squared (and is valid since we are looking at how
# the predictor variables relate, and we are not using the response)
heart_lm <- lm(as.numeric(chd) ~ age + sbp + chol + weight + cigs, data = heart)
vif(heart_lm)
max(vif(heart_lm))
min(vif(heart_lm)
## Max less than 10
## Mean less than 5
plot(heart_logistic, which = 5, cook.level = .5)
plot(heart_logistic, which = 5, cook.levels = .5)
confint(heart_logistic)
headt_conf_LOR <- confint(heart_logistic)[-1, ]
headt_conf_LOR <- confint(heart_logistic)[-1, ]
exp(heart_conf_int_LOR)
headt_conf_LOR <- confint(heart_logistic)[-1, ]
exp(heart_conf_LOR)
heart_conf_LOR <- confint(heart_logistic)[-1, ]
exp(heart_conf_LOR)
heart_conf_LOR <- confint(heart_logistic)[-1, ]
exp(heart_conf_LOR)
100 * (1 - exp(heart_conf_LOR))
heart_conf_LOR <- confint(heart_logistic)[-1, ]
exp(heart_conf_LOR)
100 * (exp(heart_conf_LOR) - 1)
new_patient <- data.frame(age = 50,
weight = 182,
sbp = 136,
chol = 253,
cigs = 20)
# predicted log odds
pred_log_odds <- predict(heart_logistic, newdata = new_patient)
# predicted probability
# can do this two-step process in one step by using type = "response" to get the
# predicted probability
new_patient <- data.frame(age = 50,
weight = 182,
sbp = 136,
chol = 253,
cigs = 20)
# predicted log odds
pred_log_odds <- predict(heart_logistic, newdata = new_patient)
pred_log_odds
# predicted probability
# can do this two-step process in one step by using type = "response" to get the
# predicted probability
new_patient <- data.frame(age = 50,
weight = 182,
sbp = 136,
chol = 253,
cigs = 20)
# predicted log odds
pred_log_odds <- predict(heart_logistic, newdata = new_patient)
pred_log_odds
# predicted probability
exp(pred_log_odds)
# can do this two-step process in one step by using type = "response" to get the
# predicted probability
new_patient <- data.frame(age = 50,
weight = 182,
sbp = 136,
chol = 253,
cigs = 20)
# predicted log odds
pred_log_odds <- predict(heart_logistic, newdata = new_patient)
pred_log_odds
# predicted probability
exp(pred_log_odds)
exp(pred_log_odds) / (1 + exp(pred_log_odds))
# can do this two-step process in one step by using type = "response" to get the
# predicted probability
pred_prob <- predict(heart_logistic,
newdata = new_patient,
type = 'response')
pred_prob
new_patient <- data.frame(age = 50,
weight = 182,
sbp = 136,
chol = 253,
cigs = 20)
# get the log odds ratio with the standard error
log_odds <- predict(heart_logistig,
newdata = new_patient,
se.fit = TRUE)
new_patient <- data.frame(age = 50,
weight = 182,
sbp = 136,
chol = 253,
cigs = 20)
# get the log odds ratio with the standard error
log_odds <- predict(heart_logistic,
newdata = new_patient,
se.fit = TRUE)
# compute the margin of error
moe <- qnorm(p = .975, lower.tail = TRUE) * log_odds$se.fit
# compute the 95% confidence interval (and point estimate) for the log odds
pred_interval <- log_odds$fit + c(-1, 0, 1) * moe
# compute the 95% confidence interval (and point estimate) for the predicted
# probability
# compute the 95% confidence interval (and point estimate) for the predicted
# probability
exp(pred_interval) / (1 + exp(pred_interval))
# Likelihood ratio test statistic
like_ratio <- heart_logistic$null.deviance - heart_logistic$deviance
# Likelihood ratio p-value
like_ratio
# Likelihood ratio p-value
pchisq(q = like_ratio,
df = length(coef(heart_logistic)) - 1,
lower.tail = FALSE)
1 - heart_logistic$deviance/heart_logistic$null.deviance
heart_preds
# get the predicted probabilities for all 757 patients:
heart_preds <- predict(heart_logistic,
type = 'response')
heart_preds
# create a sequence from 0 to 1 to represent all possible cut-off values (c)
# that we could choose:
possible_curoffs <- seq(0, 1, by = .01)
# transform heart$chd from a factor with levels "yes" and "no" to a factor with
# levels 1 and 0:
heart_binary <- ifelse(heart$chd == 'yes', 1, 0)
# create an empty vector where we will store the percent misclassified for each
# possible cut-off value we created:
percent_missclass <- rep(NA, length(possible_curoffs))
for (i in 1:length(possible_cutoffs)){
classify <- ifelse(heart_preds > possible_cutoffs[i], 1, 0)
percent_missclass[i] <- mean(classify != heart_binary)
}
# create a sequence from 0 to 1 to represent all possible cut-off values (c)
# that we could choose:
possible_cutoffs <- seq(0, 1, by = .01)
for (i in 1:length(possible_cutoffs)){
classify <- ifelse(heart_preds > possible_cutoffs[i], 1, 0)
percent_missclass[i] <- mean(classify != heart_binary)
}
percent_missclass
# percent_misclass holds the average misclassification rates for each cut-off
missclass_df <- as.data.frame(cbind(percent_missclass, possible_cutoffs))
# plot the misclassification rate against the cut-off value:
ggplot(data = missclass_df) +
geom_line(aes(x = possible_cutoffs), y = percent_missclass)
library(tidyverse)
library(vroom)
library(tidymodels)
library(yardstick)
library(doParallel)
library(embed) # target encoding
library(lightgbm)
library(bonsai)
df_train <- vroom('train.csv') # %>%
setwd("~/School Projects/StoreItem")
library(tidyverse)
library(vroom)
library(tidymodels)
library(yardstick)
library(doParallel)
library(embed) # target encoding
library(lightgbm)
library(bonsai)
df_train <- vroom('train.csv') # %>%
# filter(store == 5, item == 16)
df_test <- vroom('test.csv') # %>%
# filter(store == 5, item == 16)
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
my_recipe <- recipe(sales ~ ., data=df_train) %>%
step_date(date, features = c('dow', 'month', 'year', 'doy')) %>%
step_range(date_doy, min = 0 , max = pi) %>%
step_mutate(sinDOY = sin(date_doy), cosDOY = cos(date_doy)) %>%
step_lencode_mixed(all_nominal_predictors(), outcome = vars(sales)) %>%
step_rm(date, store, item) %>%
step_normalize(all_numeric_predictors())
prep <- prep(my_recipe)
baked <- bake(prep, new_data = NULL)
my_model <- boost_tree(tree_depth=tune(),
trees=tune(),
learn_rate=tune()) %>%
set_engine("lightgbm") %>%
set_mode("regression")
rand_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model)
tuning_grid <- crossing(trees = c(300, 400, 500),
learn_rate = c(.005, .01, .05),
tree_depth = c(2, 3, 5))
folds <- vfold_cv(df_train, v = 5, repeats = 1)
tree_tune <- tune_grid(my_model,
my_recipe,
folds,
control = control_grid(save_workflow = TRUE),
metrics = metric_set(smape),
grid = tuning_grid)
runs <- collect_metrics(tree_tune)
show_best(tree_tune, metric = 'smape')
best_tune <- tree_tune %>%
select_best("smape")
stopCluster(cl)
library(tidyverse)
library(vroom)
library(corrplot)
library(patchwork)
library(glmnet)
library(ggfortify)
dia.glm <- glm(diabetes ~ glucose + triceps + bmi + pedigree + age)
dia.glm <- glm(diabetes ~ glucose + triceps + bmi + pedigree + age, data = dia)
library(tidyverse)
library(vroom)
library(corrplot)
library(patchwork)
library(glmnet)
library(ggfortify)
dia <- vroom('Diabetes.txt') %>%
select(-row) %>%
mutate(diabetes = as.factor(diabetes))
summary(dia)
corrplot(cor(select(dia, -diabetes)))
plot_glucose <- ggplot(data = dia) +
geom_boxplot(aes(x = glucose, y = diabetes))
plot_bmi <- ggplot(data = dia) +
geom_boxplot(aes(x = bmi, y = diabetes))
plot_pedigree <- ggplot(data = dia) +
geom_boxplot(aes(x = pedigree, y = diabetes))
plot_age <- ggplot(data = dia) +
geom_boxplot(aes(x = age, y = diabetes))
(plot_glucose + plot_bmi) / (plot_pedigree + plot_age)
plot_glucose <- ggplot(data = dia, aes(x = glucose, y = diabetes)) +
geom_point() +
geom_jitter(height = 0.1)
plot_bmi <- ggplot(data = dia, aes(x = bmi, y = diabetes)) +
geom_point() +
geom_jitter(height = 0.1)
plot_pedigree <- ggplot(data = dia, aes(x = pedigree, y = diabetes)) +
geom_point() +
geom_jitter(height = 0.1)
plot_age <- ggplot(data = dia, aes(x = age, y = diabetes)) +
geom_point() +
geom_jitter(height = 0.1)
(plot_glucose + plot_bmi) / (plot_pedigree + plot_age)
df_x <- as.matrix(select(dia, -diabetes)) # predictors
df_y <- unlist(select(dia, diabetes)) # response
# use cross validation to pick the "best" (based on MSE) lambda
df_LASSO_cv <- cv.glmnet(x = df_x, # automatically includes a column of ones for the intercept FYI
y = df_y,
type.measure = "auc",
alpha = 1,
family = 'binomial')
# plot (log) lambda vs MSE
autoplot(df_LASSO_cv, label = FALSE) +
theme_bw() +
theme(aspect.ratio = 1)
# lambda.min: value of lambda that gives minimum mean cross-validated error
df_LASSO_cv$lambda.min
# lambda.1se: value of lambda within 1 standard error of the minimum
# cross-validated error
df_LASSO_cv$lambda.1se
coef(df_LASSO_cv, s = "lambda.min")
coef(df_LASSO_cv, s = "lambda.1se")
dia.glm <- glm(diabetes ~ glucose + triceps + bmi + pedigree + age, data = dia)
dia.glm <- glm(diabetes ~ glucose + triceps + bmi + pedigree + age, data = dia, family = binomial(link = 'logit))
summary(dia.glm)
dia.glm <- glm(diabetes ~ glucose + triceps + bmi + pedigree + age, data = dia, family = binomial(link = 'logit'))
summary(dia.glm)
dia.glm <- glm(diabetes ~ glucose + bmi + pedigree + age, data = dia, family = binomial(link = 'logit'))
summary(dia.glm)
scatter.smooth(x = dia$glucose, y = as.numeric(dia$glucose) - 1)
scatter.smooth(x = dia$bmi, y = as.numeric(dia$glucose) - 1)
scatter.smooth(x = dia$pedigree, y = as.numeric(dia$glucose) - 1)
scatter.smooth(x = dia$age, y = as.numeric(dia$glucose) - 1)
scatter.smooth(x = dia$glucose, y = as.numeric(dia$glucose) - 1)
scatter.smooth(x = dia$bmi, y = as.numeric(dia$bmi) - 1)
scatter.smooth(x = dia$pedigree, y = as.numeric(dia$pedigree) - 1)
scatter.smooth(x = dia$age, y = as.numeric(dia$age) - 1)
scatter.smooth(x = dia$glucose, y = as.numeric(dia$diabetes) - 1)
scatter.smooth(x = dia$bmi, y = as.numeric(dia$bmi) - 1)
scatter.smooth(x = dia$pedigree, y = as.numeric(dia$pedigree) - 1)
scatter.smooth(x = dia$age, y = as.numeric(dia$age) - 1)
scatter.smooth(x = dia$glucose, y = as.numeric(dia$diabetes) - 1)
scatter.smooth(x = dia$bmi, y = as.numeric(dia$diabetes) - 1)
scatter.smooth(x = dia$pedigree, y = as.numeric(dia$diabetes) - 1)
scatter.smooth(x = dia$age, y = as.numeric(dia$diabetes) - 1)
plot(heart_logistic, which = 5, cook.levels = .5)
plot(dia, which = 5, cook.levels = .5)
plot(dia.glm, which = 5, cook.levels = .5)
vif(dia.glm)
